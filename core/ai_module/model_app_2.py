from pathlib import Path
from typing import Optional, Literal
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig

MODEL_PATH_DEFAULT: Path = Path("./.trained_models")

"""
–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LoRA fine-tuned):
- BroneBonBon/Conflict-Generator-Mistral
- BroneBonBon/Conflict-Generator-Phi

–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã):
- mistralai/Mistral-7B-Instruct-v0.2
- microsoft/Phi-3-mini-4k-instruct
"""

MODEL_BASE_MAPPING = {
    "BroneBonBon/Conflict-Generator-Mistral": "mistralai/Mistral-7B-Instruct-v0.2",
    "BroneBonBon/Conflict-Generator-Phi": "microsoft/Phi-3-mini-4k-instruct",
}

BASE_MODEL_NAME_DEFAULT: str = "BroneBonBon/Conflict-Generator-Phi"


class AiModule:
    def __init__(
        self,
        model_path: Path = MODEL_PATH_DEFAULT,
        model_name: str = BASE_MODEL_NAME_DEFAULT,
        use_4bit: bool = True,
        load_mode: Literal["finetuned", "base_with_lora", "base_only"] = "finetuned"
    ) -> None:
        self._model_path = model_path
        self._model_name = model_name
        self._use_4bit = use_4bit
        self._load_mode = load_mode
        self._model = None
        self._tokenizer = None
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._base_model_name = MODEL_BASE_MAPPING.get(model_name, model_name)
        self._load_model()

    def _load_model(self) -> None:
        try:
            print(f"üîß –†–µ–∂–∏–º –∑–∞–≥—Ä—É–∑–∫–∏: {self._load_mode}")
            print(f"üì¶ –ú–æ–¥–µ–ª—å: {self._model_name}")
            print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self._device}")
            
            tokenizer_name = self._model_name if self._load_mode == "finetuned" else self._base_model_name
            print(f"üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {tokenizer_name}")
            
            self._tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_name,
                cache_dir=self._model_path,
                trust_remote_code=True
            )
        
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            if "Phi-3" in self._model_name or "phi" in self._model_name.lower():
                self._tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|user|>\n' + message['content'] + '<|end|>\n' }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>\n' + message['content'] + '<|end|>\n' }}{% else %}{{ '<|system|>\n' + message['content'] + '<|end|>\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}"
            elif "Mistral" in self._model_name or "mistral" in self._model_name.lower():
                self._tokenizer.chat_template = "{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% endif %}{% endfor %}"
            
            quantization_config = None
            if self._use_4bit and self._device == "cuda":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
            
            if self._load_mode == "finetuned":
                print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ fine-tuned –º–æ–¥–µ–ª–∏...")
                if quantization_config and self._device == "cuda":
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        cache_dir=self._model_path,
                        trust_remote_code=True
                    )
                else:
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._model_name,
                        cache_dir=self._model_path,
                        torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                        trust_remote_code=True
                    )
                    if self._device == "cpu":
                        self._model.to(self._device)
                        
            elif self._load_mode == "base_with_lora":
                print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self._base_model_name}")
                
                if quantization_config and self._device == "cuda":
                    base_model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        cache_dir=self._model_path,
                        trust_remote_code=True
                    )
                else:
                    base_model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        cache_dir=self._model_path,
                        torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                        trust_remote_code=True
                    )
                    if self._device == "cpu":
                        base_model.to(self._device)
                
                print(f"üîó –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∏–∑: {self._model_name}")
                self._model = PeftModel.from_pretrained(
                    base_model,
                    self._model_name,
                    cache_dir=self._model_path
                )
                
            else:  # base_only
                print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ fine-tuning: {self._base_model_name}")
                if quantization_config and self._device == "cuda":
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        cache_dir=self._model_path,
                        trust_remote_code=True
                    )
                else:
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        cache_dir=self._model_path,
                        torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                        trust_remote_code=True
                    )
                    if self._device == "cpu":
                        self._model.to(self._device)
            
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def get_response(
        self, 
        prompt: str, 
        max_new_tokens: int = 512,
        temperature: float = 0.8,
        top_p: float = 0.9,
        top_k: int = 50,
        use_chat_template: bool = False  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ False –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    ) -> str:
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–∞
            if use_chat_template and hasattr(self._tokenizer, 'chat_template') and self._tokenizer.chat_template:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat template –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                messages = [
                    {"role": "user", "content": prompt}
                ]
                inputs = self._tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_tensors="pt",
                    truncation=True,
                    max_length=2048
                ).to(self._device)
            else:
                # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ prompt - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                if not isinstance(prompt, str):
                    prompt = str(prompt)
                
                # –î–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ Mistral/Phi –¥–æ–±–∞–≤–∏–º –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                if "Mistral" in self._base_model_name or "mistral" in self._base_model_name.lower():
                    formatted_prompt = f"<s>[INST] {prompt} [/INST]"
                elif "Phi-3" in self._base_model_name or "phi" in self._base_model_name.lower():
                    formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
                else:
                    formatted_prompt = prompt
                
                inputs = self._tokenizer(
                    formatted_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=2048,
                    padding=True
                ).input_ids.to(self._device)
            
            if not isinstance(inputs, torch.Tensor):
                if isinstance(inputs, dict) and 'input_ids' in inputs:
                    inputs = inputs['input_ids'].to(self._device)
                else:
                    raise ValueError(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç inputs: {type(inputs)}")
            
            with torch.no_grad():
                outputs = self._model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    do_sample=True,
                    pad_token_id=self._tokenizer.pad_token_id,
                    eos_token_id=self._tokenizer.eos_token_id,
                    use_cache=True
                )
            
            full_response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if use_chat_template:
                response = self._extract_assistant_response(full_response)
            else:
                if "<|assistant|>" in full_response and "<|end|>" in full_response:
                    start = full_response.find("<|assistant|>") + len("<|assistant|>")
                    end = full_response.find("<|end|>", start)
                    response = full_response[start:end].strip() if end != -1 else full_response[start:].strip()
                elif "[/INST]" in full_response:
                    response = full_response.split("[/INST]")[-1].strip()
                else:
                    response = full_response[len(prompt):].strip() if len(full_response) > len(prompt) else full_response.strip()
            
            return response if response else "[–ú–æ–¥–µ–ª—å –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –æ—Ç–≤–µ—Ç]"
            
        except Exception as e:
            return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}]"

    def _extract_assistant_response(self, full_text: str) -> str:
        markers = ['<|assistant|>', 'assistant\n', 'Assistant:', 'ASSISTANT:']
        
        for marker in markers:
            if marker in full_text:
                parts = full_text.split(marker, 1)
                if len(parts) > 1:
                    response = parts[1].split('<|end|>')[0].strip()
                    return response
        
        return full_text.strip()

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        return {
            "model_name": self._model_name,
            "base_model": self._base_model_name,
            "load_mode": self._load_mode,
            "device": self._device,
            "quantized_4bit": self._use_4bit,
            "vocab_size": len(self._tokenizer) if self._tokenizer else 0,
            "model_type": self._model.config.model_type if self._model else "unknown"
        }

TUNED_MODEL = AiModule(
    model_name="BroneBonBon/Conflict-Generator-Mistral",
    use_4bit=True,
    load_mode="finetuned"  
)

