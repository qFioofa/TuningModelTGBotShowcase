from pathlib import Path
from typing import Optional, Literal
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig

MODEL_PATH_DEFAULT: Path = Path("./.trained_models")

"""
–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LoRA fine-tuned):
- BroneBonBon/Conflict-Generator-Mistral
- BroneBonBon/Conflict-Generator-Phi

–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã):
- mistralai/Mistral-7B-Instruct-v0.2
- microsoft/Phi-3-mini-4k-instruct
"""

# –ú–∞–ø–ø–∏–Ω–≥ fine-tuned –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Ö –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
MODEL_BASE_MAPPING = {
    "BroneBonBon/Conflict-Generator-Mistral": "mistralai/Mistral-7B-Instruct-v0.2",
    "BroneBonBon/Conflict-Generator-Phi": "microsoft/Phi-3-mini-4k-instruct",
}

BASE_MODEL_NAME_DEFAULT: str = "BroneBonBon/Conflict-Generator-Phi"


class AiModule:
    def __init__(
        self,
        model_path: Path = MODEL_PATH_DEFAULT,
        model_name: str = BASE_MODEL_NAME_DEFAULT,
        use_4bit: bool = True,
        load_mode: Literal["finetuned", "base_with_lora", "base_only"] = "finetuned"
    ) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è AI
        
        Args:
            model_path: –ü—É—Ç—å –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (fine-tuned –∏–ª–∏ –±–∞–∑–æ–≤–æ–π)
            use_4bit: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 4-–±–∏—Ç–Ω—É—é –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
            load_mode: 
                - "finetuned": –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ HF
                - "base_with_lora": –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑–æ–≤—É—é + –ø—Ä–∏–º–µ–Ω–∏—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
                - "base_only": —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ fine-tuning
        """
        self._model_path = model_path
        self._model_name = model_name
        self._use_4bit = use_4bit
        self._load_mode = load_mode
        self._model = None
        self._tokenizer = None
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._base_model_name = MODEL_BASE_MAPPING.get(model_name, model_name)
        self._load_model()

    def _load_model(self) -> None:
        try:
            print(f"üîß –†–µ–∂–∏–º –∑–∞–≥—Ä—É–∑–∫–∏: {self._load_mode}")
            print(f"üì¶ –ú–æ–¥–µ–ª—å: {self._model_name}")
            print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self._device}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            tokenizer_name = self._model_name if self._load_mode == "finetuned" else self._base_model_name
            print(f"üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {tokenizer_name}")
            
            self._tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_name,
                cache_dir=self._model_path,
                trust_remote_code=True
            )
        
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ pad_token –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ chat template –¥–ª—è Phi-3 –º–æ–¥–µ–ª–∏
            if "Phi-3" in self._model_name or "phi" in self._model_name.lower():
                self._tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|user|>\n' + message['content'] + '<|end|>\n' }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>\n' + message['content'] + '<|end|>\n' }}{% else %}{{ '<|system|>\n' + message['content'] + '<|end|>\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}"
            elif "Mistral" in self._model_name or "mistral" in self._model_name.lower():
                self._tokenizer.chat_template = "{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% endif %}{% endfor %}"
            
            # ... –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –≤–∞—à–µ–≥–æ –∫–æ–¥–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
            quantization_config = None
            if self._use_4bit and self._device == "cuda":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            if self._load_mode == "finetuned":
                # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ fine-tuned –º–æ–¥–µ–ª–∏...")
                if quantization_config and self._device == "cuda":
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        cache_dir=self._model_path,
                        trust_remote_code=True
                    )
                else:
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._model_name,
                        cache_dir=self._model_path,
                        torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                        trust_remote_code=True
                    )
                    if self._device == "cpu":
                        self._model.to(self._device)
                        
            elif self._load_mode == "base_with_lora":
                # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ + LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
                print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self._base_model_name}")
                
                if quantization_config and self._device == "cuda":
                    base_model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        cache_dir=self._model_path,
                        trust_remote_code=True
                    )
                else:
                    base_model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        cache_dir=self._model_path,
                        torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                        trust_remote_code=True
                    )
                    if self._device == "cpu":
                        base_model.to(self._device)
                
                print(f"üîó –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∏–∑: {self._model_name}")
                self._model = PeftModel.from_pretrained(
                    base_model,
                    self._model_name,
                    cache_dir=self._model_path
                )
                
            else:  # base_only
                # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ fine-tuning
                print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ fine-tuning: {self._base_model_name}")
                if quantization_config and self._device == "cuda":
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        cache_dir=self._model_path,
                        trust_remote_code=True
                    )
                else:
                    self._model = AutoModelForCausalLM.from_pretrained(
                        self._base_model_name,
                        cache_dir=self._model_path,
                        torch_dtype=torch.float16 if self._device == "cuda" else torch.float32,
                        trust_remote_code=True
                    )
                    if self._device == "cpu":
                        self._model.to(self._device)
            
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            print("\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
            print("  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É")
            print("  2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: transformers, peft, bitsandbytes")
            print("  3. –î–ª—è CPU –ø–æ–ø—Ä–æ–±—É–π—Ç–µ use_4bit=False")
            print("  4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace Hub")
            raise

    def get_response(
        self, 
        prompt: str, 
        max_new_tokens: int = 512,
        temperature: float = 0.8,
        top_p: float = 0.9,
        top_k: int = 50,
        use_chat_template: bool = False  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ False –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    ) -> str:
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–∞
            if use_chat_template and hasattr(self._tokenizer, 'chat_template') and self._tokenizer.chat_template:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat template –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                messages = [
                    {"role": "user", "content": prompt}
                ]
                inputs = self._tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_tensors="pt",
                    truncation=True,
                    max_length=2048
                ).to(self._device)
            else:
                # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ prompt - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                if not isinstance(prompt, str):
                    prompt = str(prompt)
                
                # –î–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ Mistral/Phi –¥–æ–±–∞–≤–∏–º –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                if "Mistral" in self._base_model_name or "mistral" in self._base_model_name.lower():
                    formatted_prompt = f"<s>[INST] {prompt} [/INST]"
                elif "Phi-3" in self._base_model_name or "phi" in self._base_model_name.lower():
                    formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
                else:
                    formatted_prompt = prompt
                
                inputs = self._tokenizer(
                    formatted_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=2048,
                    padding=True
                ).input_ids.to(self._device)
            
            # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ inputs - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä PyTorch
            if not isinstance(inputs, torch.Tensor):
                # –ï—Å–ª–∏ —ç—Ç–æ dict, –≤–æ–∑—å–º–µ–º input_ids
                if isinstance(inputs, dict) and 'input_ids' in inputs:
                    inputs = inputs['input_ids'].to(self._device)
                else:
                    raise ValueError(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç inputs: {type(inputs)}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = self._model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    do_sample=True,
                    pad_token_id=self._tokenizer.pad_token_id,
                    eos_token_id=self._tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            full_response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–º—Ç)
            if use_chat_template:
                response = self._extract_assistant_response(full_response)
            else:
                # –î–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
                if "<|assistant|>" in full_response and "<|end|>" in full_response:
                    # Phi-3 —Ñ–æ—Ä–º–∞—Ç
                    start = full_response.find("<|assistant|>") + len("<|assistant|>")
                    end = full_response.find("<|end|>", start)
                    response = full_response[start:end].strip() if end != -1 else full_response[start:].strip()
                elif "[/INST]" in full_response:
                    # Mistral —Ñ–æ—Ä–º–∞—Ç
                    response = full_response.split("[/INST]")[-1].strip()
                else:
                    response = full_response[len(prompt):].strip() if len(full_response) > len(prompt) else full_response.strip()
            
            return response if response else "[–ú–æ–¥–µ–ª—å –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –æ—Ç–≤–µ—Ç]"
            
        except Exception as e:
            return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}]"

    def _extract_assistant_response(self, full_text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –î–ª—è Phi-3 –∏ Mistral –æ—Ç–≤–µ—Ç –∏–¥–µ—Ç –ø–æ—Å–ª–µ <|assistant|> –∏–ª–∏ –ø–æ–¥–æ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        markers = ['<|assistant|>', 'assistant\n', 'Assistant:', 'ASSISTANT:']
        
        for marker in markers:
            if marker in full_text:
                parts = full_text.split(marker, 1)
                if len(parts) > 1:
                    response = parts[1].split('<|end|>')[0].strip()
                    return response
        
        # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        return full_text.strip()

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        return {
            "model_name": self._model_name,
            "base_model": self._base_model_name,
            "load_mode": self._load_mode,
            "device": self._device,
            "quantized_4bit": self._use_4bit,
            "vocab_size": len(self._tokenizer) if self._tokenizer else 0,
            "model_type": self._model.config.model_type if self._model else "unknown"
        }


def chat_simulation(ai_module: AiModule) -> None:
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å –º–æ–¥–µ–ª—å—é"""
    print("\n" + "="*60)
    print("ü§ñ Conflict Generator - –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫")
    print("="*60)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    info = ai_module.get_model_info()
    print(f"\nüìä –ú–æ–¥–µ–ª—å: {info['model_name']}")
    print(f"üèóÔ∏è  –ë–∞–∑–æ–≤–∞—è: {info['base_model']}")
    print(f"üîß –†–µ–∂–∏–º: {info['load_mode']}")
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {info['device']}")
    print(f"‚ö° –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è 4-bit: {info['quantized_4bit']}")
    print(f"üìù –¢–∏–ø –º–æ–¥–µ–ª–∏: {info['model_type']}")
    print(f"\nüí° –ö–æ–º–∞–Ω–¥—ã:")
    print("  - 'exit' - –≤—ã—Ö–æ–¥")
    print("  - 'help' - —Å–ø—Ä–∞–≤–∫–∞")
    print("  - 'info' - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏")
    print("  - 'example' - –ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n")
    
    while True:
        try:
            user_input = input("\nüë§ –í—ã: ").strip()
            
            if not user_input:
                continue
                
            if user_input.lower() == "exit":
                print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
                break
                
            if user_input.lower() == "help":
                print("\nüìñ –°–ø—Ä–∞–≤–∫–∞ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:")
                print("  –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏.")
                print("  –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:")
                print("    - '—Å–æ–∑–¥–∞–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –≤ IT —Å—Ñ–µ—Ä–µ'")
                print("    - '–Ω–∞–ø–∏—à–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—É—é —Å–∏—Ç—É–∞—Ü–∏—é –º–µ–∂–¥—É –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º'")
                print("    - '—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –°–∏—Ç—É–∞—Ü–∏—è, –¢–∏–ø, –ê—Ç–∞–∫–∞, –ó–∞—â–∏—Ç–∞'")
                continue
                
            if user_input.lower() == "info":
                info = ai_module.get_model_info()
                print("\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
                for key, value in info.items():
                    print(f"  {key}: {value}")
                continue
            
            if user_input.lower() == "example":
                example = "–Ω–∞–ø–∏—à–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç –≤ IT —Å—Ñ–µ—Ä–µ. –í—ã–≤–µ–¥–∏ –µ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –°–∏—Ç—É–∞—Ü–∏—è:\\n–¢–∏–ø:\\n–ê—Ç–∞–∫–∞:\\n–ó–∞—â–∏—Ç–∞:\\n"
                print(f"\nüí° –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞: {example}")
                user_input = example
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            print("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...", end="", flush=True)
            response = ai_module.get_response(user_input)
            print("\r" + " "*30 + "\r", end="")  # –û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏
            print(f"ü§ñ AI:\n{response}")
            
        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    try:
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Conflict Generator...")
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        # –í–ê–ñ–ù–û: –ú–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã!
        # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∂–∏–º "base_with_lora"
        
        # 1. Conflict Generator Phi (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):
        ai = AiModule(
            model_name="BroneBonBon/Conflict-Generator-Mistral",
            use_4bit=True,
            load_mode="finetuned"  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û ‚Äî —ç—Ç–æ full model
        )
        
        # 2. Conflict Generator Mistral:
        # ai = AiModule(
        #     model_name="BroneBonBon/Conflict-Generator-Mistral",
        #     use_4bit=True,
        #     load_mode="base_with_lora"  # ‚Üê –ü–†–ê–í–ò–õ–¨–ù–´–ô –†–ï–ñ–ò–ú
        # )
        
        # 3. –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ fine-tuning (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è):
        # ai = AiModule(
        #     model_name="microsoft/Phi-3-mini-4k-instruct",
        #     use_4bit=True,
        #     load_mode="base_only"
        # )
        
        chat_simulation(ai)
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())